{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = pd.read_csv('m5-forecasting-accuracy/sales_train_evaluation.csv')\n",
    "sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar = pd.read_csv('m5-forecasting-accuracy/calendar.csv')\n",
    "calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt sales data (convert from wide to long format)\n",
    "sales_long = sales.melt(id_vars=[\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"], var_name=\"d\", value_name=\"y\")\n",
    "\n",
    "# Merge with calendar to get actual dates\n",
    "sales_long = sales_long.merge(calendar[[\"d\", \"date\"]], on=\"d\", how=\"left\")\n",
    "\n",
    "# Create unique_id\n",
    "sales_long[\"unique_id\"] = sales_long[\"item_id\"] + \"_\" + sales_long[\"store_id\"]\n",
    "\n",
    "# Rename columns to match StemGNN requirements\n",
    "sales_long = sales_long.rename(columns={\"date\": \"ds\"})[[\"unique_id\", \"ds\", \"y\"]]\n",
    "\n",
    "# Sort data\n",
    "sales_long = sales_long.sort_values(by=[\"unique_id\", \"ds\"])\n",
    "\n",
    "sales_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_long.to_csv(\"m5_stemgnn.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_30_consecutive_zeros(y_values):\n",
    "    count = 0\n",
    "    for val in y_values:\n",
    "        if val == 0:\n",
    "            count += 1\n",
    "            if count >= 30:\n",
    "                return True  # Found 30 consecutive zeros\n",
    "        else:\n",
    "            count = 0  # Reset counter\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sales_long.copy()\n",
    "# Identify `unique_id`s to remove\n",
    "ids_to_remove = df.groupby(\"unique_id\")[\"y\"].apply(has_30_consecutive_zeros)\n",
    "ids_to_remove = ids_to_remove[ids_to_remove].index  # Get unique_ids to drop\n",
    "ids_to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter dataset\n",
    "df_filtered = df[~df[\"unique_id\"].isin(ids_to_remove)]\n",
    "\n",
    "# Save cleaned dataset\n",
    "df_filtered.to_csv(\"m5_stemgnn_filtered.csv\", index=False)\n",
    "\n",
    "print(f\"Removed {len(ids_to_remove)} unique_id(s) with 30 consecutive zeros.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_daily_to_weekly(input_csv_path, output_csv_path):\n",
    "    \"\"\"\n",
    "    Converts a daily sales dataset to weekly by summing sales over each week.\n",
    "    The first day of the week (Saturday) is used as the timestamp.\n",
    "    \n",
    "    Parameters:\n",
    "    input_csv_path (str): Path to the input daily CSV file.\n",
    "    output_csv_path (str): Path to save the output weekly CSV file.\n",
    "    \"\"\"\n",
    "    # Load daily data\n",
    "    daily_df = pd.read_csv(input_csv_path)\n",
    "    daily_df['ds'] = pd.to_datetime(daily_df['ds'])\n",
    "    \n",
    "    # Load calendar to get Walmart week numbers\n",
    "    calendar = pd.read_csv('m5-forecasting-accuracy/calendar.csv')\n",
    "    calendar['date'] = pd.to_datetime(calendar['date'])\n",
    "    \n",
    "    # Merge with calendar to obtain 'wm_yr_wk' (Walmart week identifier)\n",
    "    merged = daily_df.merge(\n",
    "        calendar[['date', 'wm_yr_wk']],\n",
    "        left_on='ds', \n",
    "        right_on='date', \n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Create mapping from Walmart week to the first day of the week (Saturday)\n",
    "    week_start_map = calendar.groupby('wm_yr_wk')['date'].min().reset_index()\n",
    "    week_start_map.columns = ['wm_yr_wk', 'week_start_date']\n",
    "    \n",
    "    # Aggregate sales by unique_id and Walmart week\n",
    "    weekly_sales = merged.groupby(['unique_id', 'wm_yr_wk'])['y'].sum().reset_index()\n",
    "    \n",
    "    # Add week start date (Saturday) as the new timestamp\n",
    "    weekly_sales = weekly_sales.merge(\n",
    "        week_start_map, \n",
    "        on='wm_yr_wk', \n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Format final output\n",
    "    weekly_sales = weekly_sales[['unique_id', 'week_start_date', 'y']]\n",
    "    weekly_sales.rename(columns={'week_start_date': 'ds'}, inplace=True)\n",
    "    weekly_sales.sort_values(['unique_id', 'ds'], inplace=True)\n",
    "    \n",
    "    # Save weekly dataset\n",
    "    weekly_sales.to_csv(output_csv_path, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all datasets to weekly format\n",
    "datasets = [\n",
    "    ('m5_stemgnn.csv', 'm5_stemgnn_weekly.csv'),\n",
    "    ('m5_stemgnn_filtered.csv', 'm5_stemgnn_filtered_weekly.csv'),\n",
    "]\n",
    "\n",
    "for input_file, output_file in datasets:\n",
    "    convert_daily_to_weekly(input_file, output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
