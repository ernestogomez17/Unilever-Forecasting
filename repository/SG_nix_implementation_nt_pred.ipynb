{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J8jLQyRX7ElA",
    "outputId": "d1cc9ce6-b7ec-40f7-c732-d7d55571685a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import extrametrics as em\n",
    "from neuralforecast.core import NeuralForecast\n",
    "from neuralforecast.losses.pytorch import MSE, MAE, MAPE, RMSE\n",
    "from neuralforecast.losses.numpy import mse, mae, mape, rmse\n",
    "from neuralforecast.models import StemGNN\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "HeUnfBFP7ElB",
    "outputId": "dcb75f86-7a10-4678-82f5-f9a4011eee58"
   },
   "outputs": [],
   "source": [
    "Y_df = pd.read_csv('m5_stemgnn_smallest_weekly.csv')\n",
    "Y_df['ds'] = pd.to_datetime(Y_df['ds'])\n",
    "Y_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l6FWIV8T7ElC",
    "outputId": "54258a9e-64db-46db-bc6c-9efeebbf2657"
   },
   "outputs": [],
   "source": [
    "minmax_scaler = MinMaxScaler()\n",
    "\n",
    "# Step 2: Group by unique_id to normalize each product's timeseries individually\n",
    "def apply_minmax(group):\n",
    "    # MinMax scaling\n",
    "    group['y_minmax'] = minmax_scaler.fit_transform(group[['y']])\n",
    "    return group\n",
    "\n",
    "\n",
    "# Apply MinMax normalization\n",
    "Y_df_minmax = Y_df.groupby('unique_id', group_keys=False).apply(apply_minmax)\n",
    "\n",
    "\n",
    "# Drop the original 'y' column and rename the normalized columns\n",
    "Y_df_minmax = Y_df_minmax.drop(columns=['y']).rename(columns={'y_minmax': 'y'})\n",
    "\n",
    "print(Y_df_minmax.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8FD_-3aC7ElC",
    "outputId": "343b16f5-0b26-4d02-a73b-9cca0e808840"
   },
   "outputs": [],
   "source": [
    "# We make validation and test splits\n",
    "n_time = len(Y_df.ds.unique())\n",
    "val_size = int(.2 * n_time)\n",
    "test_size = int(.1 * n_time)\n",
    "timeseries_count = len(Y_df.unique_id.unique())\n",
    "\n",
    "n_time, val_size, test_size, timeseries_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VUlHNEi77ElD",
    "outputId": "5573d906-d0ae-4f68-8bff-cb6a1cf6ff8f"
   },
   "outputs": [],
   "source": [
    "model = StemGNN(\n",
    "        h=13,\n",
    "        input_size=13,\n",
    "        n_series=timeseries_count,\n",
    "        scaler_type='robust',\n",
    "        max_steps=100,\n",
    "        early_stop_patience_steps=-1,\n",
    "        val_check_steps=1,\n",
    "        learning_rate=1e-2,\n",
    "        loss=MAE(),\n",
    "        valid_loss=None,\n",
    "        batch_size=8,\n",
    "        random_seed = 1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hGWDMuQV7ElF"
   },
   "outputs": [],
   "source": [
    "def metrics_eval_grid(model, Y_hat_df):\n",
    "    mae_model = mae(Y_hat_df['y'], Y_hat_df[f'{model}'])\n",
    "    mse_model = mse(Y_hat_df['y'], Y_hat_df[f'{model}'])\n",
    "    mape_model = mape(Y_hat_df['y'], Y_hat_df[f'{model}'])\n",
    "    rmse_model = rmse(Y_hat_df['y'], Y_hat_df[f'{model}'])\n",
    "    wmape_model = em.wmape(Y_hat_df['y'], Y_hat_df[f'{model}'])\n",
    "    r_squared_model = em.r_squared(Y_hat_df['y'], Y_hat_df[f'{model}'])\n",
    "\n",
    "    return mae_model, mse_model, rmse_model, mape_model, wmape_model, r_squared_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 311
    },
    "id": "E_BfW0PwByAC",
    "outputId": "a88f98c9-02eb-4b87-dff8-eb7bfe29b81d"
   },
   "outputs": [],
   "source": [
    "# Step 1: Create and fit the model\n",
    "nf = NeuralForecast(models=[model], freq='W')\n",
    "Y_hat_df = nf.cross_validation(df=Y_df_minmax, val_size=val_size, test_size=test_size, n_windows=None)                                 \n",
    "Y_hat_df = Y_hat_df.reset_index()\n",
    "Y_hat_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_mae, y_mse, y_rmse, y_mape, y_wmape, y_r_squared = metrics_eval_grid(model, Y_hat_df)\n",
    "print(f\"MAE: {y_mae:.3f}, MSE: {y_mse:.3f}, RMSE: {y_rmse:.3f}, \"\n",
    "                  f\"MAPE: {y_mape:.3f}%, WMAPE: {y_wmape:.3f}%, R_Squared: {y_r_squared:.3f}%\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './checkpoints/' \n",
    "nf.save(path=path, model_index=None, overwrite=True, save_dataset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize output dataframe with weekly frequency\n",
    "output_df = Y_df.copy()\n",
    "HORIZON = 13  # 13 weeks\n",
    "n_series = Y_df['unique_id'].nunique()\n",
    "\n",
    "# Find common window range across all series\n",
    "group_sizes = Y_df_minmax.groupby('unique_id').size()\n",
    "max_window_start = group_sizes.min() - 2 * HORIZON\n",
    "window_starts = range(max_window_start + 1)\n",
    "n_windows = len(window_starts)\n",
    "\n",
    "# Create weekly aligned windows using sliding_window_view\n",
    "window_data = []\n",
    "for uid, group in Y_df_minmax.groupby('unique_id'):\n",
    "    weekly_dates = group['ds'].values\n",
    "    y_values = group['y'].values\n",
    "    \n",
    "    # Create weekly windows\n",
    "    windows = np.lib.stride_tricks.sliding_window_view(y_values, 2 * HORIZON)[:n_windows]\n",
    "    \n",
    "    window_data.append({\n",
    "        'uid': uid,\n",
    "        'input_windows': windows[:, :HORIZON],\n",
    "        'pred_windows': windows[:, HORIZON:],\n",
    "        'input_dates': [weekly_dates[i:i+HORIZON] for i in window_starts],\n",
    "        'pred_dates': [weekly_dates[i+HORIZON:i+2*HORIZON] for i in window_starts]\n",
    "    })\n",
    "\n",
    "# Create batched prediction dataframe with weekly alignment\n",
    "predict_records = []\n",
    "for w in window_data:\n",
    "    for i in range(n_windows):\n",
    "        for week_idx in range(HORIZON):\n",
    "            predict_records.append({\n",
    "                'unique_id': f\"{w['uid']}_window_{i}\",\n",
    "                'ds': w['input_dates'][i][week_idx],  # Maintain weekly dates\n",
    "                'y': w['input_windows'][i, week_idx]\n",
    "            })\n",
    "\n",
    "window_df = pd.DataFrame(predict_records)\n",
    "Y_hat = nf.predict(window_df)\n",
    "pred_lookup = Y_hat.groupby('unique_id')['StemGNN'].apply(list).to_dict()\n",
    "\n",
    "# Process predictions with weekly alignment\n",
    "scaler_dict = Y_df.groupby('unique_id')['y'].agg(['min', 'max']).to_dict('index')\n",
    "output_df = output_df.set_index(['unique_id', 'ds'])\n",
    "\n",
    "for w in tqdm(window_data, desc='Weekly Windows'):\n",
    "    uid = w['uid']\n",
    "    scaler = scaler_dict[uid]\n",
    "    \n",
    "    for i in range(n_windows):\n",
    "        col_X = f'X_{i}'\n",
    "        col_Y = f'Y_{i}'\n",
    "        \n",
    "        # Input values\n",
    "        input_dates = w['input_dates'][i]\n",
    "        output_df.loc[(uid, input_dates), col_X] = Y_df.loc[\n",
    "            (Y_df['unique_id'] == uid) & (Y_df['ds'].isin(input_dates)), 'y'\n",
    "        ].values\n",
    "        \n",
    "        # Predictions\n",
    "        pred_key = f\"{uid}_window_{i}\"\n",
    "        preds = np.array(pred_lookup.get(pred_key, [np.nan]*HORIZON))\n",
    "        denorm_preds = preds * (scaler['max'] - scaler['min']) + scaler['min']\n",
    "        denorm_preds = np.round(np.clip(denorm_preds, 0, None)).astype(int)\n",
    "        \n",
    "        pred_dates = w['pred_dates'][i]\n",
    "        output_df.loc[(uid, pred_dates), col_Y] = denorm_preds[:len(pred_dates)]\n",
    "\n",
    "# Final formatting\n",
    "output_df = output_df.reset_index()\n",
    "output_df['ds'] = output_df['ds'].dt.strftime('%Y-%m-%d')\n",
    "output_df = output_df[[c for c in output_df.columns if not c.startswith('level')]]\n",
    "\n",
    "output_df.to_csv('weekly_output.csv', index=False)\n",
    "print(output_df.head())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
